{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9be44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Came for lunch with my sister. We loved our Thai-style mains which were amazing with lots of flavour, very impressive for a vegetarian restaurant.\n",
    "\n",
    "But the service was below average and the chips were too terrible to finish.\n",
    "\n",
    "When we arrived at 1.40, we had to wait 20 minutes while they got our table ready. OK, so we didn't have a reservation, but the restaurant was only half full. There was no reason to make us wait at all.\n",
    "\n",
    "We ordered the chips as a side dish and they looked delicious. But, when we tasted them, they were overcooked and swimming in oil so we left most of them. We expected a lot more for $10!\n",
    "\n",
    "When the waiter asked if everything was ok, we said we really didn't like the chips and he said 'That's funny, I love them' and that was it. He didn't offer us anything else or take them off our bill. Also, when we didn't leave a tip, he looked annoyed.\n",
    "\n",
    "I was really excited about visiting Vega, and the mains were just fantastic, but the rest of the experience was really disappointing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c74160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd2d48",
   "metadata": {},
   "source": [
    "## Breaking Paragraph at sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51cf0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Came for lunch with my sister.\n",
      "We loved our Thai-style mains which were amazing with lots of flavour, very impressive for a vegetarian restaurant.\n",
      "But the service was below average and the chips were too terrible to finish.\n",
      "When we arrived at 1.40, we had to wait 20 minutes while they got our table ready.\n",
      "OK, so we didn't have a reservation, but the restaurant was only half full.\n",
      "There was no reason to make us wait at all.\n",
      "We ordered the chips as a side dish and they looked delicious.\n",
      "But, when we tasted them, they were overcooked and swimming in oil so we left most of them.\n",
      "We expected a lot more for $10!\n",
      "When the waiter asked if everything was ok, we said we really didn't like the chips and he said 'That's funny, I love them' and that was it.\n",
      "He didn't offer us anything else or take them off our bill.\n",
      "Also, when we didn't leave a tip, he looked annoyed.\n",
      "I was really excited about visiting Vega, and the mains were just fantastic, but the rest of the experience was really disappointing.\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157061f",
   "metadata": {},
   "source": [
    "### Using regular expression for dropping all text except capital and small characters and numbersfrom 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797846d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' came for lunch with my sister ', 'we loved our thai style mains which were amazing with lots of flavour  very impressive for a vegetarian restaurant ', 'but the service was below average and the chips were too terrible to finish ', 'when we arrived at 1 40  we had to wait 20 minutes while they got our table ready ', 'ok  so we didn t have a reservation  but the restaurant was only half full ', 'there was no reason to make us wait at all ', 'we ordered the chips as a side dish and they looked delicious ', 'but  when we tasted them  they were overcooked and swimming in oil so we left most of them ', 'we expected a lot more for  10 ', 'when the waiter asked if everything was ok  we said we really didn t like the chips and he said  that s funny  i love them  and that was it ', 'he didn t offer us anything else or take them off our bill ', 'also  when we didn t leave a tip  he looked annoyed ', 'i was really excited about visiting vega  and the mains were just fantastic  but the rest of the experience was really disappointing ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = []\n",
    "for sent in sentences:\n",
    "    corpus.append(re.sub('[^a-zA-Z0-9]', \" \", sent).lower())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81f4f0",
   "metadata": {},
   "source": [
    "## Printing each doc in corpus separately just to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8af086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  came for lunch with my sister \n",
      "1 we loved our thai style mains which were amazing with lots of flavour  very impressive for a vegetarian restaurant \n",
      "2 but the service was below average and the chips were too terrible to finish \n",
      "3 when we arrived at 1 40  we had to wait 20 minutes while they got our table ready \n",
      "4 ok  so we didn t have a reservation  but the restaurant was only half full \n",
      "5 there was no reason to make us wait at all \n",
      "6 we ordered the chips as a side dish and they looked delicious \n",
      "7 but  when we tasted them  they were overcooked and swimming in oil so we left most of them \n",
      "8 we expected a lot more for  10 \n",
      "9 when the waiter asked if everything was ok  we said we really didn t like the chips and he said  that s funny  i love them  and that was it \n",
      "10 he didn t offer us anything else or take them off our bill \n",
      "11 also  when we didn t leave a tip  he looked annoyed \n",
      "12 i was really excited about visiting vega  and the mains were just fantastic  but the rest of the experience was really disappointing \n"
     ]
    }
   ],
   "source": [
    "for doc_number, corp in enumerate(corpus):\n",
    "    print(doc_number, corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6508e84",
   "metadata": {},
   "source": [
    "## Use of stemmer to convert the original word to base word and filtering stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f278f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['came lunch sister ',\n",
       " 'love thai style main amaz lot flavour impress vegetarian restaur ',\n",
       " 'servic averag chip terribl finish ',\n",
       " 'arriv 1 40 wait 20 minut got tabl readi ',\n",
       " 'ok reserv restaur half full ',\n",
       " 'reason make us wait ',\n",
       " 'order chip side dish look delici ',\n",
       " 'tast overcook swim oil left ',\n",
       " 'expect lot 10 ',\n",
       " 'waiter ask everyth ok said realli like chip said funni love ',\n",
       " 'offer us anyth els take bill ',\n",
       " 'also leav tip look annoy ',\n",
       " 'realli excit visit vega main fantast rest experi realli disappoint ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_corpus = []\n",
    "stop_words = []\n",
    "# separating words and stop words and applying stemming on words.\n",
    "for sent in corpus:\n",
    "    stemmed_doc = \"\"\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            stemmed_doc +=stemmer.stem(word)+ \" \"\n",
    "        else:\n",
    "            stop_words.append(word)\n",
    "    stemmed_corpus.append(stemmed_doc)\n",
    "stemmed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7ed25",
   "metadata": {},
   "source": [
    "## all the words other-than stop words after applying stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de0da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "came lunch sister  love thai style main amaz lot flavour impress vegetarian restaur  servic averag chip terribl finish  arriv 1 40 wait 20 minut got tabl readi  ok reserv restaur half full  reason make us wait  order chip side dish look delici  tast overcook swim oil left  expect lot 10  waiter ask everyth ok said realli like chip said funni love  offer us anyth els take bill  also leav tip look annoy  realli excit visit vega main fantast rest experi realli disappoint  "
     ]
    }
   ],
   "source": [
    "for stemmedword in stemmed_corpus:\n",
    "    print(stemmedword, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807df15a",
   "metadata": {},
   "source": [
    "## Example to unserstand what a stemmer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236f8b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"history\")\n",
    "#base word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d177a",
   "metadata": {},
   "source": [
    "## all the filtered stop words after applying stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c5cef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for with my we our which were with of very for a but the was below and the were too to when we at we had to while they our so we didn t have a but the was only there was no to at all we the as a and they but when we them they were and in so we most of them we a more for when the if was we we didn t the and he that s i them and that was it he didn t or them off our when we didn t a he i was about and the were just but the of the was "
     ]
    }
   ],
   "source": [
    "for stopwords in stop_words:\n",
    "    print(stopwords,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5885140",
   "metadata": {},
   "source": [
    "## Using lemmatizer to get the meaningful word of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e41b4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d2051d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['came lunch sister ',\n",
       " 'love thai style main amaz lot flavour impress vegetarian restaur ',\n",
       " 'servic averag chip terribl finish ',\n",
       " 'arriv 1 40 wait 20 minut got tabl readi ',\n",
       " 'ok reserv restaur half full ',\n",
       " 'reason make us wait ',\n",
       " 'order chip side dish look delici ',\n",
       " 'tast overcook swim oil left ',\n",
       " 'expect lot 10 ',\n",
       " 'waiter ask everyth ok said realli like chip said funni love ',\n",
       " 'offer us anyth els take bill ',\n",
       " 'also leav tip look annoy ',\n",
       " 'realli excit visit vega main fantast rest experi realli disappoint ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a6be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['came lunch sister ', 'love thai style main amaz lot flavour impress vegetarian restaur ', 'servic averag chip terribl finish ', 'arriv 1 40 wait 20 minut got tabl readi ', 'ok reserv restaur half full ', 'reason make u wait ', 'order chip side dish look delici ', 'tast overcook swim oil left ', 'expect lot 10 ', 'waiter ask everyth ok said realli like chip said funni love ', 'offer u anyth el take bill ', 'also leav tip look annoy ', 'realli excit visit vega main fantast rest experi realli disappoint ']\n"
     ]
    }
   ],
   "source": [
    "stemmed_lemmatized_corpus=[]\n",
    "for corp in stemmed_corpus:\n",
    "    lemmatized_doc = \"\"\n",
    "    words = nltk.word_tokenize(corp)\n",
    "    \n",
    "    for word in words:\n",
    "        lemmatized_doc +=lemmatizer.lemmatize(word)+ \" \"        \n",
    "    stemmed_lemmatized_corpus.append(lemmatized_doc)\n",
    "\n",
    "print(stemmed_lemmatized_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e32b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "binary_BOW_vectorizer= CountVectorizer(stop_words='english',binary=True)\n",
    "\n",
    "#we can apply ngrams methods in BOW by just passing ngram parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3371fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['came lunch sister ', 'love thai style main amaz lot flavour impress vegetarian restaur ', 'servic averag chip terribl finish ', 'arriv 1 40 wait 20 minut got tabl readi ', 'ok reserv restaur half full ', 'reason make us wait ', 'order chip side dish look delici ', 'tast overcook swim oil left ', 'expect lot 10 ', 'waiter ask everyth ok said realli like chip said funni love ', 'offer us anyth els take bill ', 'also leav tip look annoy ', 'realli excit visit vega main fantast rest experi realli disappoint ']\n"
     ]
    }
   ],
   "source": [
    "# test code now implemeted above \n",
    "from nltk.stem import PorterStemmer  # allows to redefine the word on its root word\n",
    "from nltk.corpus import stopwords    # are the words that donot have direct impact on semantics of documents \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_corpus = []\n",
    "stop_words = []\n",
    "# separating words and stop words and applying stemming on words.\n",
    "for sent in corpus:\n",
    "    doc = \"\"\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            doc +=stemmer.stem(word)+ \" \"\n",
    "            \n",
    "        else:\n",
    "            stop_words.append(word)\n",
    "    stemmed_corpus.append(doc)\n",
    "\n",
    "print(stemmed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1518e083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13x62 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = binary_BOW_vectorizer.fit_transform(stemmed_lemmatized_corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac4360df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'came': 9,\n",
       " 'lunch': 32,\n",
       " 'sister': 49,\n",
       " 'love': 31,\n",
       " 'thai': 55,\n",
       " 'style': 50,\n",
       " 'main': 33,\n",
       " 'amaz': 3,\n",
       " 'lot': 30,\n",
       " 'flavour': 21,\n",
       " 'impress': 25,\n",
       " 'vegetarian': 58,\n",
       " 'restaur': 46,\n",
       " 'servic': 48,\n",
       " 'averag': 8,\n",
       " 'chip': 10,\n",
       " 'terribl': 54,\n",
       " 'finish': 20,\n",
       " 'arriv': 6,\n",
       " '40': 2,\n",
       " 'wait': 60,\n",
       " '20': 1,\n",
       " 'minut': 35,\n",
       " 'got': 23,\n",
       " 'tabl': 52,\n",
       " 'readi': 41,\n",
       " 'ok': 38,\n",
       " 'reserv': 44,\n",
       " 'half': 24,\n",
       " 'reason': 43,\n",
       " 'make': 34,\n",
       " 'order': 39,\n",
       " 'dish': 13,\n",
       " 'look': 29,\n",
       " 'delici': 11,\n",
       " 'tast': 53,\n",
       " 'overcook': 40,\n",
       " 'swim': 51,\n",
       " 'oil': 37,\n",
       " 'left': 27,\n",
       " 'expect': 17,\n",
       " '10': 0,\n",
       " 'waiter': 61,\n",
       " 'ask': 7,\n",
       " 'everyth': 15,\n",
       " 'said': 47,\n",
       " 'realli': 42,\n",
       " 'like': 28,\n",
       " 'funni': 22,\n",
       " 'offer': 36,\n",
       " 'anyth': 5,\n",
       " 'el': 14,\n",
       " 'leav': 26,\n",
       " 'tip': 56,\n",
       " 'annoy': 4,\n",
       " 'excit': 16,\n",
       " 'visit': 59,\n",
       " 'vega': 57,\n",
       " 'fantast': 19,\n",
       " 'rest': 45,\n",
       " 'experi': 18,\n",
       " 'disappoint': 12}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary  = binary_BOW_vectorizer.vocabulary_\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee6c7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428534ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 62)\n",
      "(1, 62)\n",
      "(1, 62)\n",
      "(1, 62)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].toarray().shape)\n",
    "print(X[10].toarray().shape)\n",
    "print(X[12].toarray().shape)\n",
    "print(X[3].toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa12523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  came for lunch with my sister \n",
      "2 we loved our thai style mains which were amazing with lots of flavour  very impressive for a vegetarian restaurant \n",
      "3 but the service was below average and the chips were too terrible to finish \n",
      "4 when we arrived at 1 40  we had to wait 20 minutes while they got our table ready \n"
     ]
    }
   ],
   "source": [
    "print(1, corpus[0])\n",
    "print(2, corpus[1])\n",
    "print(3,corpus[2])\n",
    "print(4, corpus[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df5c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1\n",
      "32 1\n",
      "49 1\n"
     ]
    }
   ],
   "source": [
    "for i, code in enumerate(X[0].toarray()[0]):\n",
    "    if code==1:\n",
    "        print(i,code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3946dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nCame for lunch with my sister.',\n",
       " 'We loved our Thai-style mains which were amazing with lots of flavour, very impressive for a vegetarian restaurant.',\n",
       " 'But the service was below average and the chips were too terrible to finish.',\n",
       " 'When we arrived at 1.40, we had to wait 20 minutes while they got our table ready.',\n",
       " \"OK, so we didn't have a reservation, but the restaurant was only half full.\",\n",
       " 'There was no reason to make us wait at all.',\n",
       " 'We ordered the chips as a side dish and they looked delicious.',\n",
       " 'But, when we tasted them, they were overcooked and swimming in oil so we left most of them.',\n",
       " 'We expected a lot more for $10!',\n",
       " \"When the waiter asked if everything was ok, we said we really didn't like the chips and he said 'That's funny, I love them' and that was it.\",\n",
       " \"He didn't offer us anything else or take them off our bill.\",\n",
       " \"Also, when we didn't leave a tip, he looked annoyed.\",\n",
       " 'I was really excited about visiting Vega, and the mains were just fantastic, but the rest of the experience was really disappointing.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cecae",
   "metadata": {},
   "source": [
    "## All the above applied process in one code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af215906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['came lunch sister',\n",
       " 'love thai style main amaz lot flavour impress vegetarian restaur',\n",
       " 'servic averag chip terribl finish',\n",
       " 'arriv 1 40 wait 20 minut got tabl readi',\n",
       " 'ok reserv restaur half full',\n",
       " 'reason make u wait',\n",
       " 'order chip side dish look delici',\n",
       " 'tast overcook swim oil left',\n",
       " 'expect lot 10',\n",
       " 'waiter ask everyth ok said realli like chip said funni love',\n",
       " 'offer u anyth els take bill',\n",
       " 'also leav tip look annoy',\n",
       " 'realli excit visit vega main fantast rest experi realli disappoint']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z0-9]',' ',sentences[i]).lower().split()\n",
    "    \n",
    "    review=[lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=[stemmer.stem(word) for word in review]\n",
    "    \n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    corpus.append(review)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111fac4",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cdf340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4480943",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(stemmed_lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c135777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'came lunch sister '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_lemmatized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6ed052b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e81d2a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = TfidfVectorizer(ngram_range=(3,3))\n",
    "X = cv.fit_transform(stemmed_lemmatized_corpus)\n",
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4fb2049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = TfidfVectorizer(ngram_range=(3,3),max_features=10)\n",
    "X = cv.fit_transform(stemmed_lemmatized_corpus)\n",
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f37b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
